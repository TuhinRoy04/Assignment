{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import AveragePooling2D,Dense,Dropout,Flatten\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuhin Roy\\anaconda3\\lib\\site-packages\\PIL\\Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "base_dir=r\"dataset\"\n",
    "categories=[\"with_mask\",\"without_mask\"]\n",
    "data=[]\n",
    "labels=[]\n",
    "for cat in categories:\n",
    "    path=os.path.join(base_dir,cat)\n",
    "    for img in os.listdir(path):\n",
    "        img_path=os.path.join(path,img)\n",
    "        images=load_img(img_path,target_size=(224,224))\n",
    "        images=img_to_array(images)\n",
    "        images=preprocess_input(images)\n",
    "        data.append(images)\n",
    "        labels.append(cat)\n",
    "\n",
    "data=np.array(data,dtype=\"float32\")\n",
    "labels=np.array(labels)\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "labels=lb.fit_transform(labels)\n",
    "labels=to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(data,labels,stratify=labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "189/189 [==============================] - 283s 1s/step - loss: 0.0797 - accuracy: 0.9732 - val_loss: 0.0308 - val_accuracy: 0.9907\n",
      "Epoch 2/20\n",
      "189/189 [==============================] - 274s 1s/step - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.0269 - val_accuracy: 0.9907\n",
      "Epoch 3/20\n",
      "189/189 [==============================] - 255s 1s/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0292 - val_accuracy: 0.9914\n",
      "Epoch 4/20\n",
      "189/189 [==============================] - 256s 1s/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.0254 - val_accuracy: 0.9921\n",
      "Epoch 5/20\n",
      "189/189 [==============================] - 281s 1s/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0264 - val_accuracy: 0.9921\n",
      "Epoch 6/20\n",
      "189/189 [==============================] - 274s 1s/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0258 - val_accuracy: 0.9921\n",
      "Epoch 7/20\n",
      "189/189 [==============================] - 271s 1s/step - loss: 0.0093 - accuracy: 0.9965 - val_loss: 0.0305 - val_accuracy: 0.9927\n",
      "Epoch 8/20\n",
      "189/189 [==============================] - 251s 1s/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0391 - val_accuracy: 0.9921\n",
      "Epoch 9/20\n",
      "189/189 [==============================] - 273s 1s/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0324 - val_accuracy: 0.9901\n",
      "Epoch 10/20\n",
      "189/189 [==============================] - 263s 1s/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.0552 - val_accuracy: 0.9894\n",
      "Epoch 11/20\n",
      "189/189 [==============================] - 253s 1s/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0327 - val_accuracy: 0.9921\n",
      "Epoch 12/20\n",
      "189/189 [==============================] - 251s 1s/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0321 - val_accuracy: 0.9921\n",
      "Epoch 13/20\n",
      "189/189 [==============================] - 250s 1s/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0378 - val_accuracy: 0.9927\n",
      "Epoch 14/20\n",
      "189/189 [==============================] - 250s 1s/step - loss: 0.0071 - accuracy: 0.9974 - val_loss: 0.0542 - val_accuracy: 0.9894\n",
      "Epoch 15/20\n",
      "189/189 [==============================] - 250s 1s/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0340 - val_accuracy: 0.9914\n",
      "Epoch 16/20\n",
      "189/189 [==============================] - 253s 1s/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0519 - val_accuracy: 0.9874\n",
      "Epoch 17/20\n",
      "189/189 [==============================] - 251s 1s/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0417 - val_accuracy: 0.9894\n",
      "Epoch 18/20\n",
      "189/189 [==============================] - 251s 1s/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0351 - val_accuracy: 0.9927\n",
      "Epoch 19/20\n",
      "189/189 [==============================] - 251s 1s/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0353 - val_accuracy: 0.9927\n",
      "Epoch 20/20\n",
      "189/189 [==============================] - 250s 1s/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0361 - val_accuracy: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12ae54cb6c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseModel=MobileNetV2(input_shape=(224,224,3),include_top=False,weights='imagenet')\n",
    "headModel=baseModel.output\n",
    "headModel=AveragePooling2D(pool_size=(7,7))(headModel)\n",
    "headModel=Flatten(name=\"flatten\")(headModel)\n",
    "headModel=Dense(128,activation=\"relu\")(headModel)\n",
    "headModel=Dropout(0.5)(headModel)\n",
    "headModel=Dense(2,activation=\"softmax\")(headModel)\n",
    "model=Model(inputs=baseModel.input,outputs=headModel)\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable=False\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuhin Roy\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask_detect.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"mask_detect.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Face Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "faceCascade=cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "labels_dict={1:\"No Mask\",0:\"Mask\"}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "frame=cv2.imread(\"test_image2.jpg\")\n",
    "rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "faces=faceCascade.detectMultiScale(rgb,1.1,3)\n",
    "for (x,y,w,h) in faces:\n",
    "    face_img=frame[y:y+h,x:x+w]\n",
    "    images=cv2.resize(face_img,(224,224))\n",
    "    images=img_to_array(images)\n",
    "    images=preprocess_input(images)\n",
    "    images=np.array([images],dtype=\"float32\")\n",
    "    pred=model.predict(images)\n",
    "    label=np.argmax(pred,axis=1)[0]\n",
    "    percen=str(\" : \")+str((np.max(pred)*100).round(2))+str(\"%\")\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "    if (label==1):\n",
    "        cv2.putText(frame,labels_dict[label],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "        cv2.putText(frame,percen,(x+70,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "    else:\n",
    "        cv2.putText(frame,labels_dict[label],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),2)\n",
    "        cv2.putText(frame,percen,(x+40,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),2)\n",
    "cv2.imshow(\"Face Mask Detection in Image\",frame)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time Face Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade=cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "labels_dict={0:\"MASK\",1:\"NO MASK\"}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "cap=cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    _,img=cap.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    faces=cascade.detectMultiScale(gray,1.1,3)\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img=img[y:y+w,x:x+w]\n",
    "        image=cv2.resize(face_img,(224,224))\n",
    "        image=img_to_array(image)\n",
    "        image=preprocess_input(image)\n",
    "        image=np.array([image],dtype=\"float32\")\n",
    "        pred=model.predict(image)\n",
    "        sed=str(\" : \")+str((np.max(pred)*100).round(2))+str(\"%\")\n",
    "        label=np.argmax(pred,axis=1)[0]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        if (label==1):\n",
    "            cv2.putText(img,labels_dict[label],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "            cv2.putText(img,sed,(x+70,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),2)\n",
    "        else:\n",
    "            cv2.putText(img,labels_dict[label],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),2)\n",
    "            cv2.putText(img,sed,(x+40,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),2)\n",
    "    cv2.imshow(\"Real-time Face Mask Detection\",img)\n",
    "    if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
